# ============================================
# AutoDrama Dependencies (Final Version)
# Conflict-Free Tested Stack
# ============================================

# ============================================
# Core Python Packages
# ============================================
pyyaml>=6.0,<7.0
ffmpeg-python>=0.2.0,<0.3.0

# ============================================
# Numpy - Critical Version (vLLM Requirement)
# ============================================
numpy>=1.26.0,<2.0.0
# vLLM requires numpy<2.0
# transformers 4.45.2 compatible with numpy 1.26.x

# ============================================
# HuggingFace Ecosystem (Strictly Versioned)
# ============================================
huggingface-hub>=0.34.0,<1.0.0
# transformers 4.45.2 runtime requires huggingface-hub>=0.34.0
# vLLM 0.6.6.post1 compatible

hf-transfer>=0.1.0,<1.0.0
# Fast model downloads

transformers==4.45.2
# vLLM 0.6.6.post1 official requirement: transformers>=4.45.2
# Qwen2.5-72B requires transformers>=4.37.0
# Fixed to 4.45.2 for stability

tokenizers>=0.19.1,<0.24.0
# vLLM 0.6.6.post1 official requirement: tokenizers>=0.19.1
# transformers 4.45.2 runtime compatible with 0.22.x-0.23.x
# Range expanded to cover both requirements
# WARNING: faster-whisper requires tokenizers<0.16 (INCOMPATIBLE!)

# ============================================
# PyTorch (Installed via setup.sh with --extra-index-url)
# ============================================
# torch==2.5.1+cu124
# torchvision==0.20.1+cu124
# torchaudio==2.5.1+cu124
# NOTE: These are installed separately in setup_complete.sh
#       to ensure correct CUDA version and compatibility
# vLLM 0.6.6.post1 requires torchvision==0.20.1

# ============================================
# LLM Inference - vLLM
# ============================================
vllm==0.6.6.post1
# Requires:
#   - torch==2.5.1
#   - transformers>=4.45,<4.46
#   - tokenizers>=0.19,<0.21
#   - numpy<2.0
# All requirements satisfied above

# ============================================
# STT - Whisper-CTranslate2
# ============================================
whisper-ctranslate2>=0.4.3,<1.0.0
# IMPORTANT: Uses CTranslate2 backend
# Compatible with tokenizers 0.20.3
# Does NOT require tokenizers<0.16 (unlike faster-whisper)
# Faster-whisper EXCLUDED due to tokenizers conflict

# ============================================
# TTS - Coqui TTS
# ============================================
TTS>=0.22.0,<0.23.0
# Stable Korean TTS
# Compatible with torch 2.5.1

# ============================================
# Image Generation - SDXL Lightning
# ============================================
diffusers>=0.27.0,<0.30.0
# SDXL Lightning support
# Requires transformers (already installed)

accelerate>=0.20.0,<1.0.0
# Model acceleration and offloading

safetensors>=0.4.0,<1.0.0
# Safe model serialization

invisible-watermark>=0.2.0,<1.0.0
# SDXL watermarking

xformers==0.0.28.post3
# Memory-efficient attention
# vLLM 0.6.6.post1 official requirement
# Compatible with torch 2.5.1 + CUDA 12.4

# ============================================
# Utilities
# ============================================
Pillow>=10.0.0,<11.0.0
# Image processing

tqdm>=4.66.0,<5.0.0
# Progress bars

# ============================================
# Optional Performance Enhancements
# ============================================
# triton>=2.2.0,<3.0.0
# CUDA kernel compilation (vLLM optimization)
# Uncomment if using NVIDIA GPU with compute capability >= 7.0

# ============================================
# Dependency Resolution Summary
# ============================================
# Core conflict resolved:
#   - faster-whisper (requires tokenizers<0.16) → EXCLUDED
#   - whisper-ctranslate2 (compatible with tokenizers 0.20.3) → INCLUDED
#
# Version chain validation:
#   vLLM 0.6.6.post1 → transformers 4.45.2 → tokenizers 0.22.x ✓
#   diffusers 0.27+ → transformers 4.45.2 ✓
#   TTS 0.22+ → torch 2.5.1 ✓
#   xformers 0.0.23 → torch 2.5.1 ✓
#
# VRAM estimate:
#   - vLLM (Qwen 72B AWQ): ~36-40GB
#   - SDXL Lightning: ~6-8GB
#   - TTS: ~2-3GB
#   - Whisper large-v3: ~3-4GB
#   Total: ~47-55GB (80GB GPU recommended, 40GB minimum)
