# ==========================================
# LLM 백엔드 선택: "openai" 또는 "anthropic"
# ==========================================
LLM_PROVIDER=openai

# ==========================================
# OpenAI 호환 API 설정 (runpod, vLLM, Ollama 등)
# ==========================================
# API 키 (일부 로컬 서버는 필요 없음)
OPENAI_API_KEY=dummy

# API 베이스 URL
# runpod 예시: https://your-pod-id-8000.proxy.runpod.net/v1
# vLLM 로컬: http://localhost:8000/v1
# Ollama: http://localhost:11434/v1
OPENAI_BASE_URL=http://localhost:8000/v1

# 모델명
# Llama 3.1 70B: meta-llama/Llama-3.1-70B-Instruct
# Llama 3.1 8B: meta-llama/Llama-3.1-8B-Instruct
# Qwen 2.5 72B: Qwen/Qwen2.5-72B-Instruct
MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct

# ==========================================
# Anthropic API 설정 (Claude 사용시)
# ==========================================
# ANTHROPIC_API_KEY=your_api_key_here
# MODEL_NAME=claude-sonnet-4-5-20250929
